# Jailbreak VLM Model Configuration File - New Structure
# Supports managing multiple models under providers, models can override provider configurations

# ============ Provider Configuration ============
# Format:
# providers:
#   provider_name:
#     api_key: ${ENV_VAR}  # Read from environment variable
#     base_url: null       # Optional, default base_url
#     models:
#       model_name:
#         model_name: "Actual model name"
#         max_tokens: 1000
#         temperature: 0.0
#         base_url: "Custom URL"  # Optional, override provider's base_url
#         api_key: ${CUSTOM_KEY} # Optional, override provider's api_key

providers:
  # OpenAI Provider
  openai:
    api_key: "sk-WE6FQhZoEr4nBz4KTqHuzAYixq6pQG07VZbRhcHaWdGtMTs2"  # Write specific API key directly
    base_url: "https://api.whatai.cc/v1"  # Use OpenAI official API
    # api_key: "sk-WfpmwP9V5AmEABkeC5C435F58e9d4220A2D6070fD3AbC6Cd"
    models:
      gpt-4o-mini:
        model_name: "gpt-4o-mini"
        max_tokens: 1000
        temperature: 0.0
      gpt-5:
        model_name: "gpt-5"
        max_tokens: 1000
        temperature: 0.0
      gpt-4-turbo:
        model_name: "gpt-4-turbo"
        max_tokens: 1000
        temperature: 0.0
      gpt-3.5-turbo:
        model_name: "gpt-3.5-turbo"
        max_tokens: 1000
        temperature: 0.0

  # Google Gemini Provider
  google:
    api_key: "your-google-api-key-here"  # Write specific API key directly
    models:
      gemini-2.5-flash:
        model_name: "gemini-2.5-flash"
        max_tokens: 1000
        temperature: 0.0
      gemini-2.5-pro:
        model_name: "gemini-2.5-pro"
        max_tokens: 2000
        temperature: 0.0

  # Anthropic Claude Provider
  anthropic:
    api_key: "sk-ant-your-anthropic-api-key-here"  # Write specific API key directly
    models:
      claude-sonnet-4:
        model_name: "claude-3-5-sonnet-20241022"
        max_tokens: 4000
        temperature: 0.0
      claude-haiku:
        model_name: "claude-3-haiku-20240307"
        max_tokens: 4000
        temperature: 0.0

  # Qwen (Aliyun) Provider
  qwen:
    api_key: "your-qwen-api-key-here"  # Write specific API key directly
    models:
      qwen3-vl-plus:
        model_name: "qwen3-vl-plus"
        max_tokens: 2000
        temperature: 0.0
      qwen3-vl-flash:
        model_name: "qwen3-vl-flash"
        max_tokens: 2000
        temperature: 0.0

  # ByteDance (Doubao) Provider
  doubao:
    api_key: "your-doubao-api-key-here"  # Write specific API key directly
    models:
      doubao-vision-lite:
        model_name: "doubao-1.5-vision-lite-250315"
        max_tokens: 2000
        temperature: 0.0

  # vLLM Open Source Model Provider
  vllm:
    api_key: "dummy"  # vLLM does not require a real API key
    # base_url: "http://localhost:8000/v1"  # Default base_url
    base_url: "http://localhost:8005/v1"  # Model-level override
    models:
      # target model - YOUR LOCAL QWEN3VL
      # qwen3-vl-8b:
      #   model_name: "qwen3-vl-8b"
      #   max_tokens: 2000
      #   temperature: 0.0
      #   base_url: "http://localhost:8008/v1"  # Model-level override
      deepseek-vl2:
        model_name: "deepseek-vl2"
        max_tokens: 2000
        temperature: 0.0
      deepseek-vl2-small:
        model_name: "deepseek-vl2-small"
        max_tokens: 2000
        temperature: 0.0
      deepseek-vl2-tiny:
        model_name: "deepseek-vl2-tiny"
        max_tokens: 2000
        temperature: 0.0
      
      Qwen3-VL-8B-Instruct:
        model_name: "Qwen3-VL-8B-Instruct"
        max_tokens: 2000
        temperature: 0.0
      Qwen3-VL-8B-Thinking:
        model_name: "Qwen3-VL-8B-Thinking"
        max_tokens: 2000
        temperature: 0.0
      Qwen3-VL-30B-A3B-Instruct:
        model_name: "Qwen3-VL-30B-A3B-Instruct"
        max_tokens: 2000
        temperature: 0.0
      Qwen3-VL-30B-A3B-Thinking:
        model_name: "Qwen3-VL-30B-A3B-Thinking"
        max_tokens: 2000
        temperature: 0.0
      
      gemma-3-4b-it:
        model_name: "gemma-3-4b-it"
        max_tokens: 2000
        temperature: 0.0
      gemma-3-12b-it:
        model_name: "gemma-3-12b-it"
        max_tokens: 2000
        temperature: 0.0
      gemma-3-27b-it:
        model_name: "gemma-3-27b-it"
        max_tokens: 2000
        temperature: 0.0
      
      Kimi-VL-A3B-Instruct:
        model_name: "Kimi-VL-A3B-Instruct"
        max_tokens: 2000
        temperature: 0.0
      Kimi-VL-A3B-Thinking-2506:
        model_name: "Kimi-VL-A3B-Thinking-2506"
        max_tokens: 2000
        temperature: 0.0
      
      GLM-4.6V-Flash:
        model_name: "GLM-4.6V-Flash"
        max_tokens: 2000
        temperature: 0.0
      GLM-4.1V-9B-Thinking:
        model_name: "GLM-4.1V-9B-Thinking"
        max_tokens: 2000
        temperature: 0.0
      
      Step3-VL-10B:
        model_name: "Step3-VL-10B"
        max_tokens: 2000
        temperature: 0.0
      Youtu-VL-4B-Instruct:
        model_name: "Youtu-VL-4B-Instruct"
        max_tokens: 2000
        temperature: 0.0
      
      InternVL3_5-14B-Flash:
        model_name: "InternVL3_5-14B-Flash"
        max_tokens: 2000
        temperature: 0.0
      InternVL3_5-8B:
        model_name: "InternVL3_5-8B"
        max_tokens: 2000
        temperature: 0.0
      InternVL3_5-8B-Flash:
        model_name: "InternVL3_5-8B-Flash"
        max_tokens: 2000
        temperature: 0.0
      
      Llama-4-Scout-17B-16E-Instruct:
        model_name: "Llama-4-Scout-17B-16E-Instruct"
        max_tokens: 2000
        temperature: 0.0
      Llama-Guard-4-12B:
        model_name: "Llama-Guard-4-12B"
        max_tokens: 2000
        temperature: 0.0

      llava-onevision-qwen2-7b-ov-chat-hf:
        model_name: "llava-onevision-qwen2-7b-ov-chat-hf"
        max_tokens: 2000
        temperature: 0.0
      llava-onevision-qwen2-7b-ov-hf:
        model_name: "llava-onevision-qwen2-7b-ov-hf"
        max_tokens: 2000
        temperature: 0.0
      llava-onevision-qwen2-7b-si-hf:
        model_name: "llava-onevision-qwen2-7b-si-hf"
        max_tokens: 2000
        temperature: 0.0
      llava-v1.6-mistral-7b-hf:
        model_name: "llava-v1.6-mistral-7b-hf"
        max_tokens: 2000
        temperature: 0.0

      gpt-oss-120b:
        model_name: "gpt-oss-120b"
        max_tokens: 4096
        temperature: 0.0

      

      # defense model

      ShieldLM-14B-qwen:
        model_name: ShieldLM-14B-qwen
        max_tokens: 2000
        temperature: 0.0
        base_url: "http://localhost:8020/v1"
      guardreasoner_vl:
        model_name: guardreasoner_vl
        max_tokens: 10000
        temperature: 0.0
        base_url: "http://localhost:8021/v1"
      llavaguard:
        model_name: llavaguard
        max_tokens: 1000
        temperature: 0.0
        base_url: "http://localhost:8022/v1"
      llama_guard_3:
        model_name: llama_guard_3
        max_tokens: 1000
        temperature: 0.0
        base_url: "http://localhost:8023/v1"
      llama_guard_4:
        model_name: llama_guard_4
        max_tokens: 1000
        temperature: 0.0
        base_url: "http://localhost:8024/v1"

      # evaluation model
      # gpt-oss-120b:
      #   model_name: "gpt-oss-120b"
      #   max_tokens: 1000
      #   temperature: 0.0
      #   base_url: "http://localhost:8005/v1"

  # Any platform compatible with OpenAI
  any:
    api_key: "sk-WE6FQhZoEr4nBz4KTqHuzAYixq6pQG07VZbRhcHaWdGtMTs2"  # Write specific API key directly
    base_url: "https://api.whatai.cc/v1"  # Model-level override
    models:
      gpt-4o:
        model_name: "gpt-4o"
        max_tokens: 2000
        temperature: 0.0
      gpt-3.5-turbo:
        model_name: "gpt-3.5-turbo"
        max_tokens: 1000
        temperature: 0.0
      gpt-4o-mini:
        model_name: "gpt-4o-mini"
        max_tokens: 2000
        temperature: 0.0

# ============ Model Default Configuration ============
defaults:
  max_tokens: 1000
  temperature: 0.0
  log_level: "INFO"