import subprocess
import time
import os
import yaml
import argparse
import signal
import requests
import smtplib
from email.mime.text import MIMEText
from email.header import Header

# --- è·¯å¾„ä¸ç¯å¢ƒé…ç½® (ä¿æŒä¸ Eval ä¸€è‡´) ---
VLLM_PYTHON_PATH = "/mnt/disk1/szchen/miniconda3/envs/vllm_env/bin/python"
MODELS_ROOT = "/mnt/disk1/weights/llm"
BASE_GEN_CONFIG = "config/general_config.yaml"
BASE_MOD_CONFIG = "config/model_config.yaml"
RESPONSE_ROOT = "output/responses/None"
LOG_DIR = "logs/judge_vllm_logs"

# --- é‚®ä»¶é€šçŸ¥å‡½æ•° ---
def send_email_notification(model_name, attack_name, status, details=""):
    sender_email = "chenshunzhang823@gmail.com"
    password = "noiuuflcwrmyalbf" 
    to_email = "chenshunzhang823@gmail.com"

    subject = f"VLM HADES (Judge): {attack_name} | {model_name} - {status}"
    content = (
        f"ã€Judge ä»»åŠ¡çŠ¶æ€æ›´æ–°ã€‘\n"
        f"æ”»å‡»æ‰‹æ³•: {attack_name}\n"
        f"å¾…è¯„æµ‹æ¨¡å‹: {model_name}\n"
        f"çŠ¶æ€: {status}\n"
        f"è¯¦æƒ…: {details}\n"
        f"æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}"
    )
    msg = MIMEText(content, 'plain', 'utf-8')
    msg['Subject'] = Header(subject, 'utf-8')
    msg['From'] = f"VLM Judge Monitor <{sender_email}>"
    msg['To'] = to_email

    try:
        with smtplib.SMTP_SSL("smtp.gmail.com", 465) as server:
            server.login(sender_email, password)
            server.sendmail(sender_email, [to_email], msg.as_string())
        print(f"ğŸ“§ [{model_name}] çŠ¶æ€é‚®ä»¶å·²å‘é€")
    except Exception as e:
        print(f"âš ï¸ é‚®ä»¶å‘é€å¤±è´¥: {e}")

def run_judge_vllm_pipeline(
    attack_name,
    judge_model_name,
    base_port,
    gpu_id,
    target_models,
    response_root=RESPONSE_ROOT,
    base_gen_config=BASE_GEN_CONFIG,
    base_mod_config=BASE_MOD_CONFIG,
    models_root=MODELS_ROOT,
    vllm_python_path=VLLM_PYTHON_PATH,
):
    """
    judge_model_name: ä½œä¸ºè£åˆ¤çš„æ¨¡å‹åç§° (å¦‚ 'Llama-3-70B-Instruct')
    target_models: éœ€è¦è¢«è¯„æµ‹çš„å“åº”æ¨¡å‹åˆ—è¡¨
    """
    env = os.environ.copy()
    env["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
    os.makedirs(LOG_DIR, exist_ok=True)

    # 1. å‡†å¤‡ Judge æ¨¡å‹çš„æœåŠ¡
    judge_model_path = os.path.join(models_root, judge_model_name)
    vllm_log_file = os.path.join(LOG_DIR, f"judge_{judge_model_name}_vllm.log")
    
    # å®šä¹‰ä¸´æ—¶é…ç½®
    tmp_mod_filename = f"model_config_judge_{gpu_id}.yaml"
    tmp_gen_filename = f"general_config_judge_{gpu_id}.yaml"
    tmp_mod_path = os.path.join("config", tmp_mod_filename)
    tmp_gen_path = os.path.join("config", tmp_gen_filename)

    # --- ç¬¬ä¸€æ­¥ï¼šå¯åŠ¨ vLLM Judge æœåŠ¡ ---
    subprocess.run(f"fuser -k {base_port}/tcp", shell=True, stderr=subprocess.DEVNULL)
    vllm_log_fd = open(vllm_log_file, "w")
    tp_size = len(str(gpu_id).split(','))

    vllm_cmd = (
        f"{vllm_python_path} -m vllm.entrypoints.openai.api_server "
        f"--model {judge_model_path} --served-model-name {judge_model_name} "
        f"--port {base_port} --trust-remote-code --dtype bfloat16 "
        f"--tensor-parallel-size {tp_size} --gpu-memory-utilization 0.8 "
        f"--max-model-len 8192"
    )

    print(f"ğŸš€ å¯åŠ¨ Judge æ¨¡å‹æœåŠ¡: {judge_model_name} | Port: {base_port} | GPU: {gpu_id}")
    vllm_process = subprocess.Popen(
        vllm_cmd, shell=True, stdout=vllm_log_fd, stderr=vllm_log_fd, 
        preexec_fn=os.setsid, env=env
    )

    # å¥åº·æ£€æŸ¥
    ready = False
    for i in range(1, 61):
        try:
            if requests.get(f"http://localhost:{base_port}/v1/models", timeout=5).status_code == 200:
                ready = True; break
        except: pass
        if i % 6 == 0: print(f"â³ ç­‰å¾… Judge æœåŠ¡å°±ç»ª... ({i*10}s)")
        time.sleep(10)

    if not ready:
        print("âŒ Judge æ¨¡å‹å¯åŠ¨å¤±è´¥")
        os.killpg(os.getpgid(vllm_process.pid), signal.SIGKILL)
        return

    # --- ç¬¬äºŒæ­¥ï¼šéå†ç›®æ ‡æ¨¡å‹è¿›è¡Œè¯„æµ‹ ---
    try:
        for model in target_models:
            response_file = os.path.join(response_root, f"attack_{attack_name}_model_{model}.jsonl")
            if not os.path.exists(response_file):
                print(f"âš ï¸ è·³è¿‡ï¼šæ‰¾ä¸åˆ°å“åº”æ–‡ä»¶ {response_file}")
                continue

            print(f"\nâš–ï¸ æ­£åœ¨è¯„æµ‹æ¨¡å‹å“åº”: {model}")

            # å®šåˆ¶ä¸´æ—¶ model_config (æŒ‡å‘æœ¬åœ° Judge ç«¯å£)
            with open(base_mod_config, 'r') as f:
                m_cfg = yaml.safe_load(f)
            
            new_url = f"http://localhost:{base_port}/v1"
            # è¿™é‡Œå…³é”®ï¼šéœ€è¦ç¡®ä¿ your_pipeline æ”¯æŒé€šè¿‡é…ç½®æŒ‡å®š judge æ¨¡å‹
            # å‡è®¾é€»è¾‘æ˜¯ä¿®æ”¹ default_judge çš„é…ç½®
            if 'providers' in m_cfg and 'vllm' in m_cfg['providers']:
                m_cfg['providers']['vllm']['base_url'] = new_url
                if 'models' not in m_cfg['providers']['vllm']:
                    m_cfg['providers']['vllm']['models'] = {}
                m_cfg['providers']['vllm']['models'][judge_model_name] = {'base_url': new_url}

            with open(tmp_mod_path, 'w') as f:
                yaml.safe_dump(m_cfg, f)

            # å®šåˆ¶ä¸´æ—¶ general_config
            with open(base_gen_config, 'r') as f:
                g_cfg = yaml.safe_load(f)
            
            # g_cfg['evaluation'] = {
            #     'evaluators': ["default_judge"],
            #     'evaluator_params': {
            #         'model': judge_model_name, # æŒ‡å®šä½¿ç”¨åˆšåˆšå¯åŠ¨çš„æ¨¡å‹
            #         'base_url': new_url
            #     }
            # }
            g_cfg['evaluation'] = {
                'evaluators': ["default_judge"],
                'evaluator_params': {
                    'default_judge': {
                        'model': judge_model_name,
                        'base_url': new_url,
                        'api_key': "EMPTY",
                        'temperature': 0.0,
                        'max_tokens': 2048
                    }
                }
            }


            with open(tmp_gen_path, 'w') as f:
                yaml.safe_dump(g_cfg, f)

            # è¿è¡Œ Pipeline
            start_time = time.time()
            subprocess.run([
                "python", "run_pipeline.py",
                "--config", tmp_gen_path,
                "--model-config", tmp_mod_filename, # æ³¨æ„ï¼šè¿™é‡Œè¦ä¼ æ–‡ä»¶åè€Œéè·¯å¾„ï¼Œå–å†³äºä½  pipeline çš„å®ç°
                "--stage", "evaluation",
                "--input-file", response_file
            ], check=True, env=env)

            duration = round((time.time() - start_time) / 60, 2)
            send_email_notification(model, attack_name, "Judge Success", f"è€—æ—¶: {duration} mins")

    finally:
        # --- ç¬¬ä¸‰æ­¥ï¼šæ¸…ç†ç°åœº ---
        print(f"ğŸ§¹ å…³é—­ Judge æœåŠ¡å¹¶æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        os.killpg(os.getpgid(vllm_process.pid), signal.SIGKILL)
        vllm_log_fd.close()
        for f in [tmp_mod_path, tmp_gen_path]:
            if os.path.exists(f): os.remove(f)
        subprocess.run(f"fuser -k {base_port}/tcp", shell=True, stderr=subprocess.DEVNULL)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--attack", type=str, default="figstep")
    parser.add_argument("--judge_model", type=str, required=True, help="ç”¨ä½œè£åˆ¤çš„æ¨¡å‹æ–‡ä»¶å¤¹å")
    parser.add_argument("--port", type=int, default=8001)
    parser.add_argument("--gpu", type=str, default="0")
    parser.add_argument(
        "--models",
        type=str,
        default="",
        help="é€—å·åˆ†éš”çš„å¾…è¯„æµ‹æ¨¡å‹åˆ—è¡¨ï¼›ä¸ä¼ åˆ™ä½¿ç”¨è„šæœ¬å†…é»˜è®¤åˆ—è¡¨",
    )
    parser.add_argument("--response-root", type=str, default=RESPONSE_ROOT)
    parser.add_argument("--base-gen-config", type=str, default=BASE_GEN_CONFIG)
    parser.add_argument("--base-mod-config", type=str, default=BASE_MOD_CONFIG)
    parser.add_argument("--models-root", type=str, default=MODELS_ROOT)
    parser.add_argument("--vllm-python", type=str, default=VLLM_PYTHON_PATH)
    args = parser.parse_args()

    # å¾…è¯„æµ‹çš„åˆ—è¡¨
    # ALL_MODELS = [
    #     "gemma-3-12b-it", "gemma-3-27b-it", "gemma-3-4b-it",
    #     "GLM-4.6V-Flash", "InternVL3_5-8B", "Kimi-VL-A3B-Instruct",
    #     "llava-onevision-qwen2-7b-ov-hf", "llava-v1.6-mistral-7b-hf",
    #     "Qwen3-VL-8B-Instruct", "qwen3-vl-8b", "Step3-VL-10B",
    #     "deepseek-vl2", "Qwen3-VL-30B-A3B-Instruct"
    # ]

    # ALL_MODELS = [
    #     "gemma-3-27b-it", "deepseek-vl2", "GLM-4.1V-9B-Thinking", "Qwen3-VL-30B-A3B-Instruct", "Kimi-VL-A3B-Instruct"
    # ]

    ALL_MODELS = ["GLM-4.1V-9B-Thinking"]

    if args.models.strip():
        ALL_MODELS = [m.strip() for m in args.models.split(",") if m.strip()]

    # 2. ã€æ–°å¢é€»è¾‘ã€‘åœ¨è°ƒç”¨å‰è¿›è¡Œè·¯å¾„æ£€æŸ¥ï¼Œç­›é€‰å‡ºçœŸå®å­˜åœ¨çš„æ–‡ä»¶
    MODELS_TO_CHECK = []
    for model in ALL_MODELS:
        # æ‹¼æ¥è·¯å¾„ï¼Œæ£€æŸ¥è¯¥ attack + model çš„ç»„åˆæ˜¯å¦å­˜åœ¨
        response_file = os.path.join(args.response_root, f"attack_{args.attack}_model_{model}.jsonl")
        if os.path.exists(response_file):
            MODELS_TO_CHECK.append(model)
        else:
            # ä»…æ‰“å°æç¤ºï¼Œä¸åŠ å…¥å¾…åˆ¤å®šåˆ—è¡¨
            print(f"ğŸ“Œ [File Not Found] è·³è¿‡ {model}ï¼Œå› ä¸ºæ‰¾ä¸åˆ°æ–‡ä»¶: {response_file}")

    # 3. å¦‚æœç­›é€‰åçš„åˆ—è¡¨ä¸ä¸ºç©ºï¼Œåˆ™å¯åŠ¨ Judge æµç¨‹
    if MODELS_TO_CHECK:
        print(f"âœ… æ‰¾åˆ°ä»¥ä¸‹æœ‰æ•ˆæ¨¡å‹å¾…åˆ¤å®š (Attack: {args.attack}): {MODELS_TO_CHECK}")
        run_judge_vllm_pipeline(
            args.attack,
            args.judge_model,
            args.port,
            args.gpu,
            MODELS_TO_CHECK,
            response_root=args.response_root,
            base_gen_config=args.base_gen_config,
            base_mod_config=args.base_mod_config,
            models_root=args.models_root,
            vllm_python_path=args.vllm_python,
        )
    else:
        print(f"âŒ [Skipping] æ”»å‡»æ–¹å¼ {args.attack} ä¸‹æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯ç”¨çš„å“åº”æ–‡ä»¶ï¼Œè„šæœ¬é€€å‡ºã€‚")

    # run_judge_vllm_pipeline(args.attack, args.judge_model, args.port, args.gpu, MODELS_TO_CHECK)
