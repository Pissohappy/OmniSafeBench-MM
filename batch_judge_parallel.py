import subprocess
import time
import os
import yaml
import shutil
import argparse

# --- é…ç½®åŒº ---
# åŸºç¡€è·¯å¾„é…ç½®
BASE_CONFIG = "config/general_config.yaml"
RESPONSE_ROOT = "output/responses/None" # æ¨ç†ç»“æœæ ¹ç›®å½•
LOG_DIR = "logs/eval_parallel_logs"
MODEL_NAMES = [
    # "deepseek-vl2", 
    "Qwen3-VL-8B-Instruct", 
    # "gemma-3-4b-it", 
    # "Qwen3-VL-30B-A3B-Instruct",
    "Kimi-VL-A3B-Instruct", 
    "GLM-4.6V-Flash",
    "Step3-VL-10B", 
    "Youtu-VL-4B-Instruct",
    "gemma-3-27b-it",
    "llava-onevision-qwen2-7b-ov-hf",
    "InternVL3_5-8B-Flash"
]

def run_attack_eval(attack_name):
    """
    ä¸ºä¸€ä¸ªç‰¹å®šçš„æ”»å‡»æ‰‹æ³•è¿è¡Œæ‰€æœ‰æ¨¡å‹çš„ Evaluation
    """
    print(f"ğŸ”¥ å¼€å§‹å¹¶è¡Œè¯„æµ‹æ”»å‡»æ‰‹æ³•: {attack_name}")
    
    # 1. ä¸ºè¯¥æ”»å‡»æ‰‹æ³•åˆ›å»ºç‹¬ç«‹çš„ä¸´æ—¶é…ç½®æ–‡ä»¶ (æ–¹æ¡ˆ1 æ ¸å¿ƒ)
    temp_config_path = f"config/temp_eval_{attack_name}.yaml"
    shutil.copy(BASE_CONFIG, temp_config_path)
    
    # 2. ä¿®æ”¹å‰¯æœ¬å†…å®¹ (å›ºå®š Judge ä¸º gpt-4o-mini)
    with open(temp_config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    config['evaluation']['evaluators'] = ["default_judge"]
    if 'evaluator_params' not in config['evaluation']:
        config['evaluation']['evaluator_params'] = {}
    config['evaluation']['evaluator_params']['default_judge'] = {"model": "gpt-4o-mini"}
    
    with open(temp_config_path, 'w') as f:
        yaml.safe_dump(config, f)

    # 3. å¾ªç¯è¯¥æ”»å‡»ä¸‹çš„æ‰€æœ‰æ¨¡å‹
    for model in MODEL_NAMES:
        response_file = os.path.join(RESPONSE_ROOT, f"attack_{attack_name}_model_{model}.jsonl")
        
        if not os.path.exists(response_file):
            print(f"âš ï¸ [{attack_name}] æ‰¾ä¸åˆ°æ¨¡å‹ {model} çš„å“åº”ç»“æœï¼Œè·³è¿‡...")
            continue

        log_file = os.path.join(LOG_DIR, f"eval_{attack_name}_{model}.log")
        
        print(f"âš–ï¸  [{attack_name}] æ­£åœ¨è¯„æµ‹æ¨¡å‹: {model}")
        
        try:
            with open(log_file, "w") as log_fd:
                subprocess.run([
                    "python", "run_pipeline.py",
                    "--config", temp_config_path,
                    "--stage", "evaluation",
                    "--input-file", response_file
                ], stdout=log_fd, stderr=log_fd, check=True)
            print(f"âœ… [{attack_name}] {model} è¯„æµ‹å®Œæˆ")
        except Exception as e:
            print(f"âŒ [{attack_name}] {model} è¿è¡Œå¤±è´¥ï¼Œè¯¦æƒ…è¯·çœ‹æ—¥å¿—: {log_file}")

    # 4. ä»»åŠ¡ç»“æŸæ¸…ç†
    if os.path.exists(temp_config_path):
        os.remove(temp_config_path)

if __name__ == "__main__":
    # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æ¥å¯åŠ¨ï¼Œæ–¹ä¾¿åœ¨å¤šä¸ªç»ˆç«¯çª—å£è¿è¡Œä¸åŒçš„æ”»å‡»
    # æ¯”å¦‚ï¼špython this_script.py --attack figstep
    parser = argparse.ArgumentParser()
    parser.add_argument("--attack", type=str, required=True, help="è¦è¯„æµ‹çš„æ”»å‡»æ‰‹æ³•åç§°")
    args = parser.parse_args()

    os.makedirs(LOG_DIR, exist_ok=True)
    run_attack_eval(args.attack)


# TEST_CASES_PATH = "/mnt/disk1/szchen/VLMBenchmark/repo/OmniSafeBench-MM/output/test_cases/hades/test_cases.jsonl"
# VLLM_PORT = 8008
# GENERAL_CONFIG = "config/general_config.yaml"
# LOG_DIR = "logs/hades/vllm_logs" # æ–°å¢æ—¥å¿—å­˜æ”¾ç›®å½•

# # --- é‚®ä»¶é€šçŸ¥é…ç½® ---
# def send_email_notification(model_name, status, details=""):
#     smtp_server = "smtp.gmail.com"
#     smtp_port = 465
#     sender_email = "chenshunzhang823@gmail.com"
#     password = "noiuuflcwrmyalbf" 
#     to_email = "chenshunzhang823@gmail.com"

#     subject = f"VLM Eval: {model_name} - {status}"
#     content = f"æ¨¡å‹: {model_name}\nçŠ¶æ€: {status}\nè¯¦æƒ…: {details}\næ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}"

#     msg = MIMEText(content, 'plain', 'utf-8')
#     msg['Subject'] = Header(subject, 'utf-8')
#     msg['From'] = f"H200 Server <{sender_email}>"
#     msg['To'] = to_email

#     try:
#         with smtplib.SMTP_SSL(smtp_server, smtp_port) as server:
#             server.login(sender_email, password)
#             server.sendmail(sender_email, [to_email], msg.as_string())
#         print(f"ğŸ“§ é€šçŸ¥é‚®ä»¶å·²å‘é€ (Status: {status})")
#     except Exception as e:
#         print(f"âš ï¸ é‚®ä»¶å‘é€å¤±è´¥: {e}")

# # --- åŠŸèƒ½å‡½æ•° ---
# def update_general_config(model_name):
#     with open(GENERAL_CONFIG, 'r') as f:
#         config = yaml.safe_load(f)
#     config['response_generation']['models'] = [model_name]
#     with open(GENERAL_CONFIG, 'w') as f:
#         yaml.safe_dump(config, f, default_flow_style=False)

# def wait_for_vllm(port, timeout=400):
#     start_time = time.time()
#     while time.time() - start_time < timeout:
#         try:
#             response = requests.get(f"http://localhost:{port}/v1/models")
#             if response.status_code == 200:
#                 return True
#         except:
#             pass
#         time.sleep(5)
#     return False

# # --- ä¸»å¾ªç¯ ---
# os.makedirs(LOG_DIR, exist_ok=True)
# print(f"ğŸ¬ å¼€å§‹æ‰¹é‡è¯„æµ‹ä»»åŠ¡ï¼Œå…± {len(MODEL_NAMES)} ä¸ªæ¨¡å‹...")

# for i, model in enumerate(MODEL_NAMES):
#     model_path = os.path.join(MODELS_ROOT, model)
#     log_file_path = os.path.join(LOG_DIR, f"{model}_vllm.log")
    
#     print(f"\n" + "="*50)
#     print(f"ğŸ“¦ [{i+1}/{len(MODEL_NAMES)}] å½“å‰æ¨¡å‹: {model}")
#     print(f"ğŸ“ vLLM æ—¥å¿—å°†å†™å…¥: {log_file_path}")
    
#     update_general_config(model)
#     tp_size = 2 if ("30B" in model or "27b" in model) else 1

#     # å…³é”®ï¼šå°† stdout å’Œ stderr é‡å®šå‘åˆ°æ–‡ä»¶
#     vllm_log_fd = open(log_file_path, "w")
#     vllm_cmd = (
#         f"{VLLM_PYTHON_PATH} -m vllm.entrypoints.openai.api_server "
#         f"--model {model_path} --served-model-name {model} "
#         f"--port {VLLM_PORT} --trust-remote-code --dtype bfloat16 "
#         f"--tensor-parallel-size {tp_size} --gpu-memory-utilization 0.8"
#     )

#     # --- ã€ä¿®æ”¹ç‚¹ 1: å¯åŠ¨ã€‘ ---
#     # ä½¿ç”¨ preexec_fn=os.setsid ä¸ºè¿™ä¸€å®¶å­è¿›ç¨‹åˆ›å»ºä¸€ä¸ªâ€œç»„ IDâ€
#     vllm_process = subprocess.Popen(
#         vllm_cmd, 
#         shell=True, 
#         stdout=vllm_log_fd, 
#         stderr=vllm_log_fd,
#         preexec_fn=os.setsid  
#     )
    
#     # vllm_process = subprocess.Popen(vllm_cmd, shell=True, stdout=vllm_log_fd, stderr=vllm_log_fd)

#     print(f"â³ æ­£åœ¨å¯åŠ¨ vLLM å¹¶åŠ è½½æƒé‡...")
#     if wait_for_vllm(VLLM_PORT):
#         print(f"âœ… æœåŠ¡å°±ç»ªï¼å¼€å§‹è¿è¡Œæ¨ç†æµæ°´çº¿...")
#         try:
#             # è¿è¡Œæ¨ç†é€»è¾‘
#             start_eval = time.time()
#             subprocess.run([
#                 "python", "run_pipeline.py",
#                 "--config", GENERAL_CONFIG,
#                 "--stage", "response_generation",
#                 "--test-cases-file", TEST_CASES_PATH
#             ], check=True)
            
#             duration = round((time.time() - start_eval) / 60, 2)
#             send_email_notification(model, "Success", f"æ¨ç†è€—æ—¶: {duration} mins")
            
#         except subprocess.CalledProcessError as e:
#             print(f"âŒ æ¨ç†å¤±è´¥: {e}")
#             send_email_notification(model, "Failed", f"Pipeline è¿è¡Œå‡ºé”™:\n{str(e)}")
#     else:
#         print(f"âŒ æ¨¡å‹å¯åŠ¨è¶…æ—¶ï¼è¯·æ£€æŸ¥æ—¥å¿—: {log_file_path}")
#         send_email_notification(model, "Timeout", "vLLM æœåŠ¡æœªèƒ½æˆåŠŸå¯åŠ¨")

#     # # æ¸…ç†
#     # print(f"ğŸ›‘ æ­£åœ¨æ¸…ç† {model} è¿›ç¨‹...")
#     # vllm_process.terminate()
#     # vllm_log_fd.close()
#     # subprocess.run(f"fuser -k {VLLM_PORT}/tcp", shell=True, stderr=subprocess.DEVNULL)
#     # time.sleep(15)

#     # --- ã€ä¿®æ”¹ç‚¹ 2: ç²¾ç¡®æ¸…ç†ã€‘ ---
#     print(f"ğŸ›‘ æ­£åœ¨ç²¾ç¡®æ¸…ç† {model} è¿›ç¨‹æ ‘...")
#     try:
#         # è·å–è¯¥è¿›ç¨‹ç»„ ID å¹¶å°†å…¶å…¨éƒ¨æ€æ‰ï¼Œè¿™æ ·ç»ä¸ä¼šè¯¯ä¼¤åˆ«äºº
#         pgid = os.getpgid(vllm_process.pid)
#         os.killpg(pgid, signal.SIGKILL) 
#     except Exception as e:
#         print(f"æ¸…ç†è¿‡ç¨‹ä¸­å‡ºç°å°å¼‚å¸¸ï¼ˆå¯èƒ½è¿›ç¨‹å·²ç»“æŸï¼‰: {e}")

#     vllm_log_fd.close()
    
#     # è¾…åŠ©æ¸…ç†ï¼šåªæ€æ‰å ç”¨ä½ æŒ‡å®šç«¯å£çš„è¿›ç¨‹ï¼ˆåŒé‡ä¿é™©ï¼‰
#     subprocess.run(f"fuser -k {VLLM_PORT}/tcp", shell=True, stderr=subprocess.DEVNULL)
    
#     # --- ã€ä¿®æ”¹ç‚¹ 3: æ˜¾å­˜å®‰å…¨ç­‰å¾…ã€‘ ---
#     # ä¸è¦å›ºå®šæ­»ç­‰ 15 ç§’ï¼Œæ”¹ç”¨æ˜¾å­˜ç›‘æ§
#     print("â³ ç­‰å¾…æ˜¾å­˜å®Œå…¨é‡Šæ”¾...")
#     for _ in range(30): # æœ€å¤šç­‰ 5 åˆ†é’Ÿ
#         # æ£€æŸ¥ GPU 0 çš„æ˜¾å­˜ï¼ˆå› ä¸ºä½  TP=2 è‡³å°‘ä¼šå  GPU 0ï¼‰
#         gpu_check = subprocess.check_output(
#             f"nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits -i 0", 
#             shell=True
#         ).decode().strip()
        
#         if int(gpu_check) < 10000: # å¦‚æœæ˜¾å­˜å ç”¨å°äº 10GB (H200 å¾ˆç©ºçš„çŠ¶æ€)
#             print(f"âœ… æ˜¾å­˜å·²é‡Šæ”¾ ({gpu_check} MiB)ï¼Œå‡†å¤‡ä¸‹ä¸€ä¸ªæ¨¡å‹ã€‚")
#             break
#         time.sleep(10)

# print("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼")
# send_email_notification("All Models", "Finished", "æ‰¹é‡è¯„æµ‹ä»»åŠ¡å…¨éƒ¨è·‘å®Œå•¦ï¼")