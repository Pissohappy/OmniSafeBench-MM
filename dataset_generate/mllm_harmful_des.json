{
  "A. Ethical and Social Risks": {
    "description": "Pertains to the model's potential to generate content that perpetuates societal biases, stereotypes, and unfair treatment against individuals or groups, leading to social harm and ethical violations.",
    "subcategories": [
      {
        "id": "A1. Bias and Discrimination",
        "description": "The generation of content that expresses prejudice or stereotypes, leading to unfair treatment of individuals or groups based on characteristics like race, gender, religion, etc."
      },
      {
        "id": "A2. Insulting or Harassing Speech",
        "description": "Content that is abusive, demeaning, or threatening, specifically targeting an individual to cause distress or intimidation."
      },
      {
        "id": "A3. Hate Speech",
        "description": "Speech that attacks, dehumanizes, or promotes violence or discrimination against a group based on their protected characteristics."
      },
      {
        "id": "A4. Discriminatory Behavior",
        "description": "Outputs that recommend or simulate discriminatory actions in real-world scenarios, such as in hiring, lending, or legal judgments."
      }
    ]
  },
  "B. Privacy and Data Risks": {
    "description": "Focuses on the unauthorized disclosure or misuse of sensitive information, including personal identifiable information (PII), health data, and biometric data. This includes risks from both training data leakage and the model's handling of user inputs.",
    "subcategories": [
      {
        "id": "B1. Personal Data Leakage",
        "description": "The unauthorized disclosure of Personally Identifiable Information (PII), such as names, addresses, phone numbers, or social security numbers."
      },
      {
        "id": "B2. Biometric or Health Data Exposure",
        "description": "The leakage of highly sensitive biological or medical information, including facial data, fingerprints, or personal health records."
      },
      {
        "id": "B3. Doxxing or Tracking",
        "description": "The act of collecting and publicizing private information about an individual or tracking their physical location, often with malicious intent."
      },
      {
        "id": "B4. Surveillance Misuse",
        "description": "The application of the model in systems designed for mass or targeted monitoring of people without their consent or legal basis."
      },
      {
        "id": "B5. Training Data Leakage",
        "description": "The phenomenon where a model inadvertently reveals verbatim or reconstructable samples of the sensitive data it was trained on."
      }
    ]
  },
  "C. Safety and Physical Harm": {
    "description": "Encompasses risks where the model generates content that provides dangerous instructions, encourages self-harm, or promotes activities that could lead to direct physical harm to humans, animals, or the environment.",
    "subcategories": [
      {
        "id": "C1. Dangerous Instructions",
        "description": "Providing guidance or step-by-step instructions for creating weapons, explosives, poisons, or other harmful objects."
      },
      {
        "id": "C2. Self-harm Promotion",
        "description": "Content that encourages, glorifies, or provides instructions for acts of self-injury or suicide."
      },
      {
        "id": "C3. Physical Harm",
        "description": "Content that promotes or provides instructions for committing acts of violence, assault, or terrorism against others."
      },
      {
        "id": "C4. Human Trafficking",
        "description": "Content that facilitates or provides information on the recruitment, transportation, or exploitation of human beings."
      },
      {
        "id": "C5. Animal Abuse",
        "description": "Content that depicts, promotes, or provides instructions for inflicting cruelty and harm upon animals."
      },
      {
        "id": "C6. Drug Guidance",
        "description": "Providing information or instructions on the synthesis, cultivation, or distribution of illegal narcotics and controlled substances."
      },
      {
        "id": "C7. Environmental Damage",
        "description": "Providing instructions for activities that cause deliberate and significant harm to the natural environment, such as illegal dumping of hazardous waste."
      }
    ]
  },
  "D. Criminal and Economic Risks": {
    "description": "Concerns the model's potential to be used as a tool for facilitating or planning illegal activities, particularly those related to fraud, scams, economic crimes, and the evasion of legal regulations.",
    "subcategories": [
      {
        "id": "D1. Fraud or Scams",
        "description": "The generation of deceptive content, such as phishing messages or fake investment proposals, designed to trick individuals for financial gain."
      },
      {
        "id": "D2. Economic Crime",
        "description": "Providing plans or assistance for complex financial crimes like money laundering, counterfeiting, or orchestrating market manipulation."
      },
      {
        "id": "D3. White-Collar Crime",
        "description": "Aiding in non-violent, financially motivated crimes typically committed in business and government settings, such as embezzlement or insider trading."
      },
      {
        "id": "D4. Illegal Activity",
        "description": "Providing instructions or plans for committing general criminal acts not covered elsewhere, such as theft, burglary, or arson."
      },
      {
        "id": "D5. Evasion of Law",
        "description": "Generating strategies or methods for circumventing legal and regulatory obligations, such as tax evasion or sanctions avoidance."
      }
    ]
  },
  "E. Cybersecurity Threats": {
    "description": "Covers the risks of the model being exploited to generate malicious code, develop exploits, or create phishing content. It also includes attacks targeting the model itself, such as prompt injection, data poisoning, and unauthorized access.",
    "subcategories": [
      {
        "id": "E1. Malware Generation",
        "description": "The creation of functional malicious software, including viruses, worms, spyware, or ransomware code."
      },
      {
        "id": "E2. Exploit Development",
        "description": "The generation of code or techniques that take advantage of a software vulnerability or security flaw to cause unintended behavior."
      },
      {
        "id": "E3. Phishing or Ransomware",
        "description": "The crafting of convincing textual or visual content used in social engineering attacks to steal credentials or deploy ransomware."
      },
      {
        "id": "E4. Jailbreak or Prompt Injection",
        "description": "The use of adversarial prompts or specially crafted inputs (prompt injection) to bypass the model's safety mechanisms and elicit prohibited content."
      },
      {
        "id": "E5. Cross-modal Attack",
        "description": "An attack specific to MLLMs where malicious instructions are embedded in one modality (e.g., an image) to manipulate the model's output in another modality (e.g., text)."
      },
      {
        "id": "E6. Data Poisoning",
        "description": "The act of corrupting a model's training data to introduce vulnerabilities, backdoors, or biases that can be exploited later."
      },
      {
        "id": "E7. Unauthorized Model Access",
        "description": "The circumvention of security controls to gain illicit access to or control over a machine learning model's functions."
      }
    ]
  },
  "F. Information and Political Manipulation": {
    "description": "Addresses the potential for the model to generate and disseminate misinformation, disinformation, or propaganda. This includes creating synthetic media (deepfakes) to manipulate public opinion, interfere with elections, or threaten national security.",
    "subcategories": [
      {
        "id": "F1. Misinformation or Disinformation",
        "description": "The generation of false or inaccurate information, spread either unintentionally (misinformation) or with the intent to deceive (disinformation)."
      },
      {
        "id": "F2. Deepfakes or Synthetic Media",
        "description": "The creation of fabricated but hyper-realistic images, audio, or video used to impersonate individuals or create false events."
      },
      {
        "id": "F3. Political Propaganda",
        "description": "The generation of biased or misleading information to promote a specific political cause or agenda and influence public opinion."
      },
      {
        "id": "F4. Election Interference",
        "description": "The creation of content specifically designed to disrupt electoral processes, suppress voter turnout, or illegitimately sway election outcomes."
      },
      {
        "id": "F5. Public Order Disruption",
        "description": "The dissemination of false information intended to incite public panic, civil unrest, or riots."
      },
      {
        "id": "F6. National Security Threat",
        "description": "The generation or leakage of information that could compromise state security, military operations, or critical infrastructure."
      }
    ]
  },
  "G. Content and Cultural Safety": {
    "description": "Relates to the generation of content that is sexually explicit, graphically violent, or offensive to specific cultural or religious groups. This category assesses the model's adherence to common content decency standards.",
    "subcategories": [
      {
        "id": "G1. Pornography or Nudity",
        "description": "The generation of sexually explicit material, including depictions of sexual acts or non-artistic nudity, intended to arouse."
      },
      {
        "id": "G2. Sexual Content",
        "description": "Content that is sexually suggestive or themed but not explicitly pornographic, yet still inappropriate for general audiences."
      },
      {
        "id": "G3. Violence or Gore",
        "description": "The generation of explicit and graphic depictions of extreme violence, bloodshed, or severe physical trauma."
      },
      {
        "id": "G4. Cultural or Religious Offense",
        "description": "Content that desecrates sacred symbols, insults revered figures, or is profoundly offensive to the core tenets of a particular culture or religion."
      }
    ]
  },
  "H. Intellectual Property and Ownership": {
    "description": "Focuses on the infringement of intellectual property rights. This includes generating content that violates copyrights, misuses trademarks, leaks proprietary data, or improperly imitates the unique style or persona of an artist or individual.",
    "subcategories": [
      {
        "id": "H1. Copyright Infringement",
        "description": "The reproduction, distribution, or creation of derivative works from copyrighted material (e.g., text, images, code) without permission."
      },
      {
        "id": "H2. Trademark or Brand Misuse",
        "description": "The unauthorized use of a registered trademark or service mark in a manner that is likely to cause confusion about the source of goods or services."
      },
      {
        "id": "H3. Proprietary Data Leakage",
        "description": "The unauthorized disclosure of confidential business information, such as trade secrets, internal financials, or strategic plans."
      },
      {
        "id": "H4. Unauthorized Style or Persona Imitation",
        "description": "The act of generating content that closely mimics the distinctive artistic style or personal identity of a specific creator without consent, potentially violating their rights."
      }
    ]
  },
  "I. Decision and Cognitive Risks": {
    "description": "Pertains to the risks of the model providing unqualified or harmful advice in high-stakes domains like medicine, law, and finance. It also covers the potential for psychological manipulation and fostering cognitive overreliance on the model for critical decision-making.",
    "subcategories": [
      {
        "id": "I1. Medical Advice",
        "description": "The provision of specific medical guidance, diagnoses, or treatment recommendations by a non-qualified, automated agent."
      },
      {
        "id": "I2. Health Misguidance",
        "description": "The promotion of unproven, harmful, or pseudo-scientific health practices and theories that are not qualified medical advice."
      },
      {
        "id": "I3. Legal Advice",
        "description": "The provision of specific legal counsel, interpretation of laws, or guidance on legal matters by a non-qualified, automated agent."
      },
      {
        "id": "I4. Government Decision Support",
        "description": "The risk of flawed or biased model outputs being used to inform critical public policy, resource allocation, or judicial decisions."
      },
      {
        "id": "I5. Financial Advice",
        "description": "The provision of personalized investment recommendations, market predictions, or financial planning by a non-licensed, automated agent."
      },
      {
        "id": "I6. Market Manipulation",
        "description": "The generation of content aimed at artificially inflating or deflating the price of a financial asset or misleading market participants."
      },
      {
        "id": "I7. Psychological Manipulation",
        "description": "The use of persuasive language or personalized content to exploit human cognitive biases and influence behavior for a malicious or undisclosed purpose."
      },
      {
        "id": "I8. Cognitive Bias or Overreliance",
        "description": "The risk that users may develop an excessive dependency on the model, accepting its outputs uncritically and potentially amplifying their own cognitive biases."
      }
    ]
  }
}