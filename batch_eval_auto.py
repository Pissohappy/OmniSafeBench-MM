import subprocess
import time
import os
import yaml
import shutil
import argparse
import signal
import requests
import smtplib
from email.mime.text import MIMEText
from email.header import Header

# --- åŸºç¡€åŸºç¡€è·¯å¾„é…ç½®ï¼ˆæ ¹æ®ä½ çš„æœåŠ¡å™¨ç¯å¢ƒæ£€æŸ¥ï¼‰ ---
VLLM_PYTHON_PATH = "/mnt/disk1/szchen/miniconda3/envs/vllm_env/bin/python"
KIMI_PYTHON_PATH = "/mnt/disk1/szchen/miniconda3/envs/kimi_env/bin/python"
MODELS_ROOT = "/mnt/disk1/weights/vlm"
BASE_GEN_CONFIG = "config/general_config.yaml"
BASE_MOD_CONFIG = "config/model_config.yaml"
LOG_DIR = "logs/parallel_vllm_logs"

# --- å¼•æ“é…ç½®æ˜ å°„ ---
# 1 ä»£è¡¨ä½¿ç”¨ V1 å¼•æ“ (Experimental)ï¼Œ0 ä»£è¡¨å¼ºåˆ¶å›é€€åˆ° V0 (Stable)
MODEL_ENGINE_MAP = {
    # "deepseek-vl2": "0",
    # "InternVL3_5-8B": "0",
    # "GLM-4.6V-Flash": "0",
    # "llava-onevision-qwen2-7b-ov-hf": "1", # LLaVA ç³»åˆ—é€šå¸¸å¯¹ V1 å…¼å®¹è¾ƒå¥½
    "default": "1" # å…¶ä»–é»˜è®¤å°è¯• V1
}

# --- æ˜¾å­˜/é•¿åº¦é…ç½®æ˜ å°„ (æ–°å¢) ---
MODEL_MAX_LEN_MAP = {
    "deepseek-vl2": 4096,
}

# --- é‚®ä»¶é€šçŸ¥å‡½æ•° ---
def send_email_notification(model_name, attack_name, status, details=""):
    smtp_server = "smtp.gmail.com"
    smtp_port = 465
    sender_email = "chenshunzhang823@gmail.com"
    password = "noiuuflcwrmyalbf"  # åº”ç”¨ä¸“ç”¨å¯†ç 
    to_email = "chenshunzhang823@gmail.com"

    subject = f"VLM HADES: {attack_name} | {model_name} - {status}"
    content = (
        f"ã€ä»»åŠ¡çŠ¶æ€æ›´æ–°ã€‘\n"
        f"æ”»å‡»æ‰‹æ³•: {attack_name}\n"
        f"æµ‹è¯•æ¨¡å‹: {model_name}\n"
        f"å½“å‰çŠ¶æ€: {status}\n"
        f"è¯¦ç»†ä¿¡æ¯: {details}\n"
        f"é€šçŸ¥æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}"
    )
    msg = MIMEText(content, 'plain', 'utf-8')
    msg['Subject'] = Header(subject, 'utf-8')
    msg['From'] = f"VLM Monitor <{sender_email}>"
    msg['To'] = to_email

    try:
        with smtplib.SMTP_SSL(smtp_server, smtp_port) as server:
            server.login(sender_email, password)
            server.sendmail(sender_email, [to_email], msg.as_string())
        print(f"ğŸ“§ [{model_name}] çŠ¶æ€é‚®ä»¶å·²å‘é€")
    except Exception as e:
        print(f"âš ï¸ é‚®ä»¶å‘é€å¤±è´¥: {e}")

def run_response_pipeline(
    attack_name,
    base_port,
    gpu_id,
    model_list,
    test_cases_file,
    base_gen_config=BASE_GEN_CONFIG,
    base_mod_config=BASE_MOD_CONFIG,
    models_root=MODELS_ROOT,
    vllm_python_path=VLLM_PYTHON_PATH,
    kimi_python_path=KIMI_PYTHON_PATH,
):
    # 1. å‡†å¤‡ç¯å¢ƒå˜é‡å’Œç›®å½•
    env = os.environ.copy()
    env["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
    os.makedirs(LOG_DIR, exist_ok=True)
    
    # 2. å®šä¹‰ä¸´æ—¶é…ç½®è·¯å¾„
    tmp_mod_filename = f"model_config_{attack_name}_{gpu_id}.yaml"
    tmp_gen_filename = f"general_config_{attack_name}_{gpu_id}.yaml"
    tmp_mod_path = os.path.join("config", tmp_mod_filename)
    tmp_gen_path = os.path.join("config", tmp_gen_filename)

    for model in model_list:
        model_path = os.path.join(models_root, model)
        vllm_log_file = os.path.join(LOG_DIR, f"{attack_name}_{model}_vllm.log")

        engine_version = MODEL_ENGINE_MAP.get(model, MODEL_ENGINE_MAP["default"])
        env["VLLM_USE_V1"] = engine_version

        current_python = kimi_python_path if model == "Kimi-VL-A3B-Instruct" else vllm_python_path
        
        print(f"\n" + "â–ˆ"*60)
        print(f"ğŸš€ æ­£åœ¨å¯åŠ¨: {model} | ç«¯å£: {base_port} | GPU: {gpu_id} | VLLM_USE_V1: {engine_version}")
        print(f"â–ˆ" + "â”"*59)

        # --- ç¬¬ä¸€æ­¥ï¼šå®šåˆ¶ model_config (ä¿®æ”¹ç«¯å£) ---
        with open(base_mod_config, 'r') as f:
            m_cfg = yaml.safe_load(f)
        
        new_url = f"http://localhost:{base_port}/v1"
        if 'providers' in m_cfg and 'vllm' in m_cfg['providers']:
            m_cfg['providers']['vllm']['base_url'] = new_url
            if 'models' in m_cfg['providers']['vllm']:
                for m_key in m_cfg['providers']['vllm']['models']:
                    m_cfg['providers']['vllm']['models'][m_key]['base_url'] = new_url
        
        with open(tmp_mod_path, 'w') as f:
            yaml.safe_dump(m_cfg, f)

        # --- ç¬¬äºŒæ­¥ï¼šå®šåˆ¶ general_config (è®¾ç½®ç”Ÿæˆé•¿åº¦é™åˆ¶) ---
        with open(base_gen_config, 'r') as f:
            g_cfg = yaml.safe_load(f)
        
        g_cfg['response_generation']['models'] = [model]
        # åœ¨è¿™é‡Œå¼ºåˆ¶æ³¨å…¥ max_tokensï¼Œé˜²æ­¢æ­»å¾ªç¯è¾“å‡º
        if 'model_kwargs' not in g_cfg['response_generation']:
            g_cfg['response_generation']['model_kwargs'] = {}
        g_cfg['response_generation']['model_kwargs']['max_tokens'] = 512
        g_cfg['response_generation']['model_kwargs']['temperature'] = 0.0 # ä¿æŒè¯„æµ‹çš„ä¸€è‡´æ€§

        with open(tmp_gen_path, 'w') as f:
            yaml.safe_dump(g_cfg, f)

        # --- ç¬¬ä¸‰æ­¥ï¼šæ¸…ç†å¹¶å¯åŠ¨ vLLM ---
        # å¯åŠ¨å‰å¼ºæ€ç«¯å£å ç”¨ï¼Œé˜²æ­¢å¯åŠ¨å¤±è´¥
        subprocess.run(f"fuser -k {base_port}/tcp", shell=True, stderr=subprocess.DEVNULL)
        
        vllm_log_fd = open(vllm_log_file, "w")
        tp_size = len(str(gpu_id).split(','))

        custom_max_len = MODEL_MAX_LEN_MAP.get(model)
        max_len_arg = f"--max-model-len {custom_max_len}" if custom_max_len else "--max-model-len 8192"
        
        # è¿™é‡Œçš„ --max-model-len 2048 æ˜¯ä¸ºäº†é˜²æ­¢æç«¯æƒ…å†µä¸‹æ¨¡å‹å ç”¨è¿‡å¤§ KV Cache
        vllm_cmd = (
            f"{current_python} -m vllm.entrypoints.openai.api_server "
            f"--model {model_path} --served-model-name {model} "
            f"--port {base_port} --trust-remote-code --dtype bfloat16 "
            f"--tensor-parallel-size {tp_size} --gpu-memory-utilization 0.8 "
            f"{max_len_arg}"
            # f"--max-model-len 8192" 
        )

        vllm_process = subprocess.Popen(
            vllm_cmd, shell=True, stdout=vllm_log_fd, stderr=vllm_log_fd, 
            preexec_fn=os.setsid, env=env
        )

        # --- ç¬¬å››æ­¥ï¼šå¥åº·æ£€æŸ¥ (ç­‰å¾…æœåŠ¡ Ready) ---
        ready = False
        print(f"â³ ç­‰å¾… vLLM æœåŠ¡å¯åŠ¨...")
        for i in range(1, 61): # æœ€å¤šç­‰å¾… 10 åˆ†é’Ÿ
            try:
                if requests.get(f"http://localhost:{base_port}/v1/models", timeout=5).status_code == 200:
                    ready = True
                    print(f"âœ… æœåŠ¡å·²å°±ç»ª (è€—æ—¶ {i*10}s)")
                    break
            except:
                pass
            if i % 6 == 0: print(f"   ...å·²ç­‰å¾… {i*10}s")
            time.sleep(10)

        # --- ç¬¬äº”æ­¥ï¼šè¿è¡Œæ¨ç†æµæ°´çº¿ ---
        if ready:
            try:
                start_time = time.time()
                subprocess.run([
                    "python", "run_pipeline.py",
                    "--config", tmp_gen_path,
                    "--model-config", tmp_mod_filename,
                    "--stage", "response_generation",
                    "--test-cases-file", test_cases_file
                ], check=True, env=env)
                
                duration = round((time.time() - start_time) / 60, 2)
                send_email_notification(model, attack_name, "Success", f"è€—æ—¶: {duration} mins")
            except Exception as e:
                print(f"âŒ æ¨ç†è¿è¡Œå¤±è´¥: {e}")
                send_email_notification(model, attack_name, "Failed", str(e))
        else:
            print(f"âŒ æœåŠ¡å¯åŠ¨è¶…æ—¶ï¼Œè·³è¿‡æ¨¡å‹ {model}")
            send_email_notification(model, attack_name, "Timeout", "vLLM å¯åŠ¨è¶…è¿‡ 10 åˆ†é’Ÿæ— å“åº”")

        # --- ç¬¬å…­æ­¥ï¼šå½»åº•æ¸…ç†ç°åœº ---
        print(f"ğŸ§¹ æ¸…ç† {model} è¿›ç¨‹åŠç«¯å£...")
        try:
            os.killpg(os.getpgid(vllm_process.pid), signal.SIGKILL)
        except:
            pass
        vllm_log_fd.close()
        subprocess.run(f"fuser -k {base_port}/tcp", shell=True, stderr=subprocess.DEVNULL)
        time.sleep(10) # ç»™æ˜¾å­˜ä¸€ç‚¹é‡Šæ”¾æ—¶é—´

    # å…¨éƒ¨ç»“æŸåæ¸…ç†ä¸´æ—¶æ–‡ä»¶
    for f in [tmp_mod_path, tmp_gen_path]:
        if os.path.exists(f): os.remove(f)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="AUTO Attack Batch Evaluator")
    parser.add_argument("--attack", type=str, default="figstep", help="æ”»å‡»åç§°")
    parser.add_argument("--port", type=int, required=True, help="èµ·å§‹ç«¯å£å·")
    parser.add_argument("--gpu", type=str, required=True, help="æŒ‡å®šçš„ GPU ID (å¦‚ 0 æˆ– 0,1)")
    parser.add_argument(
        "--models",
        type=str,
        default="",
        help="é€—å·åˆ†éš”çš„æ¨¡å‹åˆ—è¡¨ï¼›ä¸ä¼ åˆ™ä½¿ç”¨è„šæœ¬å†…é»˜è®¤åˆ—è¡¨",
    )
    parser.add_argument(
        "--test-cases-file",
        type=str,
        default="",
        help="æµ‹è¯•ç”¨ä¾‹æ–‡ä»¶è·¯å¾„ï¼›é»˜è®¤ output/test_cases/<attack>/test_cases.jsonl",
    )
    parser.add_argument("--base-gen-config", type=str, default=BASE_GEN_CONFIG)
    parser.add_argument("--base-mod-config", type=str, default=BASE_MOD_CONFIG)
    parser.add_argument("--models-root", type=str, default=MODELS_ROOT)
    parser.add_argument("--vllm-python", type=str, default=VLLM_PYTHON_PATH)
    parser.add_argument("--kimi-python", type=str, default=KIMI_PYTHON_PATH)
    args = parser.parse_args()

    # ä½ è¦è·‘çš„æ¨¡å‹åˆ—è¡¨
    MODELS_TO_RUN = [
        # "Qwen3-VL-8B-Instruct", 
        "Kimi-VL-A3B-Instruct", 
        # "GLM-4.6V-Flash",
        "GLM-4.1V-9B-Thinking",
        # "Step3-VL-10B", 

        "gemma-3-27b-it",
        # "gemma-3-12b-it",
        # "llava-onevision-qwen2-7b-ov-hf",
        # "InternVL3_5-8B",

        "Qwen3-VL-30B-A3B-Instruct",

        
        # "llava-v1.6-mistral-7b-hf"

        # "Youtu-VL-4B-Instruct", ä¸æ”¯æŒvllm
        "deepseek-vl2",  #å­˜åœ¨bug
        # "Llama-4-Scout-17B-16E-Instruct", éœ€è¦TP=2ä¸¤å¼ å¡

    ]

    if args.models.strip():
        MODELS_TO_RUN = [m.strip() for m in args.models.split(",") if m.strip()]

    test_cases_file = args.test_cases_file or f"output/test_cases/{args.attack}/test_cases.jsonl"

    run_response_pipeline(
        args.attack,
        args.port,
        args.gpu,
        MODELS_TO_RUN,
        test_cases_file,
        base_gen_config=args.base_gen_config,
        base_mod_config=args.base_mod_config,
        models_root=args.models_root,
        vllm_python_path=args.vllm_python,
        kimi_python_path=args.kimi_python,
    )
