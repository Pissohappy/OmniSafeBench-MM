import argparse
import os
import signal
import smtplib
import subprocess
import time
from email.header import Header
from email.mime.text import MIMEText

import requests
import yaml

# --- é…ç½®åŒº ---
VLLM_PYTHON_PATH = "/mnt/disk1/szchen/miniconda3/envs/vllm_env/bin/python"
MODELS_ROOT = "/mnt/disk1/weights/vlm"
MODEL_NAMES = [
    # "deepseek-vl2",
    "Qwen3-VL-8B-Instruct",
    # "gemma-3-4b-it",
    # "Qwen3-VL-30B-A3B-Instruct",
    "Kimi-VL-A3B-Instruct",
    "GLM-4.6V-Flash",
    "Step3-VL-10B",
    "Youtu-VL-4B-Instruct",
    "gemma-3-27b-it",
    "llava-onevision-qwen2-7b-ov-hf",
    "InternVL3_5-8B-Flash",
]
VLLM_PORT = 8008
GENERAL_CONFIG = "config/general_config.yaml"
LOG_DIR = "logs/vllm_logs"


# --- é‚®ä»¶é€šçŸ¥é…ç½® ---
def send_email_notification(model_name, status, details=""):
    smtp_server = "smtp.gmail.com"
    smtp_port = 465
    sender_email = "chenshunzhang823@gmail.com"
    password = "noiuuflcwrmyalbf"
    to_email = "chenshunzhang823@gmail.com"

    subject = f"VLM Eval: {model_name} - {status}"
    content = (
        f"æ¨¡å‹: {model_name}\nçŠ¶æ€: {status}\nè¯¦æƒ…: {details}\n"
        f"æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}"
    )

    msg = MIMEText(content, "plain", "utf-8")
    msg["Subject"] = Header(subject, "utf-8")
    msg["From"] = f"H200 Server <{sender_email}>"
    msg["To"] = to_email

    try:
        with smtplib.SMTP_SSL(smtp_server, smtp_port) as server:
            server.login(sender_email, password)
            server.sendmail(sender_email, [to_email], msg.as_string())
        print(f"ğŸ“§ é€šçŸ¥é‚®ä»¶å·²å‘é€ (Status: {status})")
    except Exception as e:
        print(f"âš ï¸ é‚®ä»¶å‘é€å¤±è´¥: {e}")


# --- åŠŸèƒ½å‡½æ•° ---
def update_general_config(model_name):
    with open(GENERAL_CONFIG, "r") as f:
        config = yaml.safe_load(f)
    config["response_generation"]["models"] = [model_name]
    with open(GENERAL_CONFIG, "w") as f:
        yaml.safe_dump(config, f, default_flow_style=False)


def wait_for_vllm(port, timeout=400):
    start_time = time.time()
    while time.time() - start_time < timeout:
        try:
            response = requests.get(f"http://localhost:{port}/v1/models")
            if response.status_code == 200:
                return True
        except Exception:
            pass
        time.sleep(5)
    return False


def parse_args():
    parser = argparse.ArgumentParser(
        description="æ‰¹é‡å“åº”ç”Ÿæˆè„šæœ¬ï¼ˆæ”¯æŒ attack å‚æ•°åŒ–ï¼Œé¿å…é‡å¤ç»´æŠ¤å¤šä»½è„šæœ¬ï¼‰"
    )
    parser.add_argument(
        "--attack",
        type=str,
        default="figstep",
        help="æµ‹è¯•ç”¨ä¾‹ç›®å½•åï¼Œå¦‚ figstep / sd35_figstep",
    )
    parser.add_argument(
        "--test-cases-path",
        type=str,
        default=None,
        help="å¯é€‰ï¼šæ˜¾å¼æŒ‡å®š test_cases.jsonl è·¯å¾„ï¼›é»˜è®¤æŒ‰ output/test_cases/<attack>/test_cases.jsonl ç»„è£…",
    )
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()

    test_cases_path = args.test_cases_path or os.path.join(
        "output", "test_cases", args.attack, "test_cases.jsonl"
    )

    # --- ä¸»å¾ªç¯ ---
    os.makedirs(LOG_DIR, exist_ok=True)
    print(
        f"ğŸ¬ å¼€å§‹æ‰¹é‡è¯„æµ‹ä»»åŠ¡ï¼Œå…± {len(MODEL_NAMES)} ä¸ªæ¨¡å‹ï¼Œ"
        f"attack={args.attack}, test_cases={test_cases_path}"
    )

    for i, model in enumerate(MODEL_NAMES):
        model_path = os.path.join(MODELS_ROOT, model)
        log_file_path = os.path.join(LOG_DIR, f"{args.attack}_{model}_vllm.log")

        print(f"\n" + "=" * 50)
        print(f"ğŸ“¦ [{i + 1}/{len(MODEL_NAMES)}] å½“å‰æ¨¡å‹: {model}")
        print(f"ğŸ“ vLLM æ—¥å¿—å°†å†™å…¥: {log_file_path}")

        update_general_config(model)
        tp_size = 2 if ("30B" in model or "27b" in model) else 1

        # å…³é”®ï¼šå°† stdout å’Œ stderr é‡å®šå‘åˆ°æ–‡ä»¶
        vllm_log_fd = open(log_file_path, "w")
        vllm_cmd = (
            f"{VLLM_PYTHON_PATH} -m vllm.entrypoints.openai.api_server "
            f"--model {model_path} --served-model-name {model} "
            f"--port {VLLM_PORT} --trust-remote-code --dtype bfloat16 "
            f"--tensor-parallel-size {tp_size} --gpu-memory-utilization 0.8"
        )

        vllm_process = subprocess.Popen(
            vllm_cmd,
            shell=True,
            stdout=vllm_log_fd,
            stderr=vllm_log_fd,
            preexec_fn=os.setsid,
        )

        print("â³ æ­£åœ¨å¯åŠ¨ vLLM å¹¶åŠ è½½æƒé‡...")
        if wait_for_vllm(VLLM_PORT):
            print("âœ… æœåŠ¡å°±ç»ªï¼å¼€å§‹è¿è¡Œæ¨ç†æµæ°´çº¿...")
            try:
                start_eval = time.time()
                subprocess.run(
                    [
                        "python",
                        "run_pipeline.py",
                        "--config",
                        GENERAL_CONFIG,
                        "--stage",
                        "response_generation",
                        "--test-cases-file",
                        test_cases_path,
                    ],
                    check=True,
                )

                duration = round((time.time() - start_eval) / 60, 2)
                send_email_notification(model, "Success", f"æ¨ç†è€—æ—¶: {duration} mins")

            except subprocess.CalledProcessError as e:
                print(f"âŒ æ¨ç†å¤±è´¥: {e}")
                send_email_notification(model, "Failed", f"Pipeline è¿è¡Œå‡ºé”™:\n{str(e)}")
        else:
            print(f"âŒ æ¨¡å‹å¯åŠ¨è¶…æ—¶ï¼è¯·æ£€æŸ¥æ—¥å¿—: {log_file_path}")
            send_email_notification(model, "Timeout", "vLLM æœåŠ¡æœªèƒ½æˆåŠŸå¯åŠ¨")

        print(f"ğŸ›‘ æ­£åœ¨ç²¾ç¡®æ¸…ç† {model} è¿›ç¨‹æ ‘...")
        try:
            pgid = os.getpgid(vllm_process.pid)
            os.killpg(pgid, signal.SIGKILL)
        except Exception as e:
            print(f"æ¸…ç†è¿‡ç¨‹ä¸­å‡ºç°å°å¼‚å¸¸ï¼ˆå¯èƒ½è¿›ç¨‹å·²ç»“æŸï¼‰: {e}")

        vllm_log_fd.close()

        subprocess.run(f"fuser -k {VLLM_PORT}/tcp", shell=True, stderr=subprocess.DEVNULL)

        print("â³ ç­‰å¾…æ˜¾å­˜å®Œå…¨é‡Šæ”¾...")
        for _ in range(30):
            gpu_check = subprocess.check_output(
                "nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits -i 0",
                shell=True,
            ).decode().strip()

            if int(gpu_check) < 10000:
                print(f"âœ… æ˜¾å­˜å·²é‡Šæ”¾ ({gpu_check} MiB)ï¼Œå‡†å¤‡ä¸‹ä¸€ä¸ªæ¨¡å‹ã€‚")
                break
            time.sleep(10)

    print("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼")
    send_email_notification("All Models", "Finished", f"{args.attack} æ‰¹é‡è¯„æµ‹ä»»åŠ¡å…¨éƒ¨è·‘å®Œå•¦ï¼")
